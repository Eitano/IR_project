{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f5e29792",
      "metadata": {
        "id": "f5e29792"
      },
      "source": [
        "## Welcome to our preproccessing - here we will calculate and write our indices to the disk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "938b1676",
      "metadata": {
        "id": "938b1676"
      },
      "source": [
        "### Imports & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79aa0dc3",
      "metadata": {
        "id": "79aa0dc3",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Setup",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes\n",
        "!pip install -U regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef6123fe",
      "metadata": {
        "id": "ef6123fe",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Imports",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "a24aa24b-aa75-4823-83ca-1d7deef0f0de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pyspark\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from google.cloud import storage\n",
        "\n",
        "import hashlib\n",
        "#Change the hash function so title/body/anchor terms will get the same value for the same term\n",
        "def _hash(s):\n",
        "    if s.endswith('_t') or s.endswith('_a'): s = s[:-2]\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7bee38f",
      "metadata": {
        "id": "c7bee38f",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-pyspark-import",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from graphframes import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "845b2128",
      "metadata": {
        "id": "845b2128",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-spark-version",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "07b4e22b-a252-42fb-fe46-d9050e4e7ca8",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cluster-16b3-m.c.linear-arcadia-330317.internal:39679\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>yarn</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySparkShell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f413def5460>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark\n",
        "# conf = (SparkConf()\n",
        "#     .set(\"spark.driver.maxResultSize\", \"4g\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a862da9",
      "metadata": {
        "id": "8a862da9",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-bucket_name",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Put your bucket name below and make sure you can access it without an error\n",
        "bucket_name = '207024878' \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a233c0d6",
      "metadata": {
        "id": "a233c0d6"
      },
      "source": [
        "Here, we read the entire corpus to an rdd, directly from Google Storage Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf91e73",
      "metadata": {
        "id": "caf91e73",
        "scrolled": false,
        "outputId": "c8cde8fc-f441-400b-a85c-facb71f2f7a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "full_path = \"gs://wikidata_preprocessed/*\"\n",
        "parquetFile = spark.read.parquet(full_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3766195",
      "metadata": {
        "id": "a3766195"
      },
      "source": [
        "Let's import the our inverted index class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36b19b50",
      "metadata": {
        "id": "36b19b50",
        "outputId": "327fe81b-80f4-4b3a-8894-e74720d92e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inverted_index_gcp.py\r\n"
          ]
        }
      ],
      "source": [
        "#look for the file\n",
        "%cd -q /home/dataproc\n",
        "!ls inverted_index_gcp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df61238c",
      "metadata": {
        "id": "df61238c",
        "scrolled": true,
        "outputId": "61237c2a-d91f-44e4-f6fd-7db4fb337663"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22/01/10 19:49:29 WARN org.apache.spark.SparkContext: The path /home/dataproc/inverted_index_gcp.py has been added already. Overwriting of added paths is not supported in the current version.\n"
          ]
        }
      ],
      "source": [
        "# adding our python module to the cluster\n",
        "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "120478c6",
      "metadata": {
        "id": "120478c6"
      },
      "outputs": [],
      "source": [
        "from inverted_index_gcp import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c481c9a",
      "metadata": {
        "id": "8c481c9a"
      },
      "source": [
        "# Building inverted indices:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8a2c111",
      "metadata": {
        "id": "d8a2c111"
      },
      "source": [
        "## First - index for the titles: (for binary index)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94c3461e",
      "metadata": {
        "id": "94c3461e"
      },
      "source": [
        "We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14e791ee",
      "metadata": {
        "id": "14e791ee"
      },
      "outputs": [],
      "source": [
        "doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd7b6299",
      "metadata": {
        "id": "dd7b6299"
      },
      "outputs": [],
      "source": [
        "#save pickle - dictionary of doc_id, title for the retrival \n",
        "with open('doc_title_dict.pkl', 'wb') as handle:\n",
        "    pickle.dump(doc_title_pairs.collectAsMap(), handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "118bcc82",
      "metadata": {
        "id": "118bcc82"
      },
      "outputs": [],
      "source": [
        "# Count number of wiki pages\n",
        "parquetFile.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65b625c2",
      "metadata": {
        "id": "65b625c2"
      },
      "source": [
        "### Tokenize "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c7b2f88",
      "metadata": {
        "id": "7c7b2f88"
      },
      "outputs": [],
      "source": [
        "# find regex pattern s\n",
        "\n",
        "def get_html_pattern():\n",
        "    pattern = \"<(\\\"[^\\\"]\\\"|'[^']|[^'\\\">])*>\"\n",
        "    return pattern\n",
        "\n",
        "def get_time_pattern():\n",
        "    pattern = \"((?:[01][0-2]|2[0-4])(?:\\.)?(?:[0-4][0-9])|(?:[0-1]?[0-9]|2[0-5]):(?:([0-5][0-9])):(?:([0-9][0-9]))?$)((AM|am|a\\.m\\.|PM|p\\.m\\.)?$|([AP][M]|[ap]\\.[m]\\.))\"\n",
        "    return pattern\n",
        "\n",
        "def get_number_pattern():\n",
        "    pattern =\"(?<![\\w\\+\\-,\\.])[\\+\\-]?\\d{1,3}((,\\d{3})|\\d)(\\.\\d+)?(?!\\S?[\\w\\+\\-])\"\n",
        "    return pattern\n",
        " \n",
        "def get_percent_pattern():\n",
        "    pattern =\"(?<![\\w\\+\\-,\\.])[\\+\\-]?\\d{1,3}((,\\d{3})|\\d)(\\.\\d+)?%(?!\\S?[\\w\\+\\-])\"\n",
        "    return pattern\n",
        "\n",
        "def get_date_pattern():\n",
        "    pattern = \"((([12][0-9]|(30)|[1-9])\\ )?(Apr(il?)?|Jun(e?)?|Sep(tember?)?|Nov(ember?)?)(\\ ([12][0-9],|(30,)|[1-9],))?((\\ \\d\\d\\d\\d)))|((Jan(uary?)?|Mar(ch?)?|May?|Jul(y?)?|Aug(ust?)?|Oct(ober?)?|Dec(ember?)?)(\\ ([12][0-9],|3[10],|[1-9],))?((\\ \\d\\d\\d\\d)))|((([1][0-9]|2[0-8]|[0-9])\\ )?(Feb(ruary?)?)(\\ ([1][0-9],|2[0-8],|[0-9],))?((\\ \\d\\d\\d\\d)))\"\n",
        "    return pattern\n",
        "\n",
        "def get_word_pattern():\n",
        "    pattern = \"(\\w+(?:-\\w+)+)|(?<!-)(\\w+'?\\w*)\"\n",
        "    return pattern\n",
        "\n",
        "\n",
        "RE_TOKENIZE = re.compile(rf\"\"\"\n",
        "(\n",
        "    # parsing html tags\n",
        "     (?P<HTMLTAG>{get_html_pattern()})                                  \n",
        "    # dates\n",
        "    |(?P<DATE>{get_date_pattern()})\n",
        "    # time\n",
        "    |(?P<TIME>{get_time_pattern()})\n",
        "    # Percents\n",
        "    |(?P<PERCENT>{get_percent_pattern()})\n",
        "    # Numbers\n",
        "    |(?P<NUMBER>{get_number_pattern()})\n",
        "    # Words\n",
        "    |(?P<WORD>{get_word_pattern()})\n",
        "    # space\n",
        "    |(?P<SPACE>[\\s\\t\\n]+) \n",
        "    # everything else\n",
        "    |(?P<OTHER>.)\n",
        ")\n",
        "\"\"\", \n",
        "re.MULTILINE | re.IGNORECASE | re.VERBOSE | re.UNICODE)\n",
        "\n",
        "def filter_text(text):\n",
        "    filtered = [v for match in RE_TOKENIZE.finditer(text)\n",
        "                 for k, v in match.groupdict().items() \n",
        "                  if v is not None and k not in ['HTMLTAG', 'DATE', 'TIME', 'PERCENT','NUMBER', 'SPACE', 'OTHER']]\n",
        "    return filtered\n",
        "\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text: string , represting the text to tokenize.    \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    list of tokens (e.g., list of tokens).\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    ls_lower = filter_text(' '.join(tokens))\n",
        "    list_of_tokens = [token for token in ls_lower if token not in all_stopwords]    \n",
        "    return list_of_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb69f91",
      "metadata": {
        "id": "9bb69f91"
      },
      "source": [
        "### Word counts - Term frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "464d5c5a",
      "metadata": {
        "id": "464d5c5a"
      },
      "outputs": [],
      "source": [
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "\n",
        "def word_count(text, id):\n",
        "    ''' Count the frequency of each word in `text` (tf) that is not included in \n",
        "  `all_stopwords` and return entries that will go into our posting lists. \n",
        "  Parameters:\n",
        "  -----------\n",
        "    text: str\n",
        "      Text of one document\n",
        "    id: int\n",
        "      Document id\n",
        "  Returns:\n",
        "  --------\n",
        "    List of tuples\n",
        "      A list of (token, (doc_id, tf)) pairs \n",
        "      for example: [(\"Anarchism\", (12, 5)), ...]\n",
        "  '''\n",
        "    tokens = tokenize(text)\n",
        "    return [(term, (id, tf)) for (term,tf) in Counter(tokens).items()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "205126e7",
      "metadata": {
        "id": "205126e7"
      },
      "outputs": [],
      "source": [
        "word_counts_title = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35763831",
      "metadata": {
        "id": "35763831"
      },
      "source": [
        "### Posting list - Reduce and sort word count to get the posting list :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4e685b3",
      "metadata": {
        "id": "c4e685b3"
      },
      "outputs": [],
      "source": [
        "def sort_word_counts(unsorted_pl):\n",
        "    ''' Returns a sorted posting list by wiki_id.\n",
        "    Parameters:\n",
        "    -----------\n",
        "    unsorted_pl: list of tuples\n",
        "      A list of (wiki_id, tf) tuples \n",
        "    Returns:\n",
        "    --------\n",
        "    list of tuples\n",
        "      A sorted posting list.\n",
        "    '''\n",
        "    return sorted(unsorted_pl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "291f7b4a",
      "metadata": {
        "id": "291f7b4a"
      },
      "outputs": [],
      "source": [
        "postings_title = word_counts_title.groupByKey().mapValues(sort_word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78366b10",
      "metadata": {
        "id": "78366b10"
      },
      "source": [
        "### add _t at the end of title tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fac3315",
      "metadata": {
        "id": "6fac3315"
      },
      "outputs": [],
      "source": [
        "postings_filtered_title = postings_title\n",
        "postings_filtered_title = postings_filtered_title.map(lambda x:(x[0]+'_t', x[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db038163",
      "metadata": {
        "id": "db038163"
      },
      "source": [
        "### Document frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "453ca5ab",
      "metadata": {
        "id": "453ca5ab"
      },
      "outputs": [],
      "source": [
        "def calculate_df(postings):\n",
        "    ''' Takes a posting list RDD and calculate the df for each token.\n",
        "    Parameters:\n",
        "    -----------\n",
        "    postings: RDD\n",
        "      An RDD where each element is a (token, posting_list) pair.\n",
        "    Returns:\n",
        "    --------\n",
        "    RDD\n",
        "      An RDD where each element is a (token, df) pair.\n",
        "    '''\n",
        "    return postings.mapValues(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b6fe03e",
      "metadata": {
        "id": "6b6fe03e"
      },
      "outputs": [],
      "source": [
        "# get df\n",
        "w2df_title = calculate_df(postings_filtered_title)\n",
        "w2df_title_dict = w2df_title.collectAsMap()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d2fafd8",
      "metadata": {
        "id": "5d2fafd8"
      },
      "source": [
        "### partition_postings_and_write"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e77367f",
      "metadata": {
        "id": "0e77367f"
      },
      "outputs": [],
      "source": [
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "    return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "def partition_postings_and_write(postings, ftype=\"\"):\n",
        "    ''' A function that partitions the posting lists into buckets, writes out \n",
        "    all posting lists in a bucket to disk, and returns the posting locations for \n",
        "    each bucket. Partitioning should be done through the use of `token2bucket` \n",
        "    above. Writing to disk should use the function  `write_a_posting_list`, a \n",
        "    static method implemented in inverted_index_colab.py under the InvertedIndex \n",
        "    class. \n",
        "    Parameters:\n",
        "    -----------\n",
        "    postings: RDD\n",
        "      An RDD where each item is a (w, posting_list) pair.\n",
        "    Returns:\n",
        "    --------\n",
        "    RDD\n",
        "      An RDD where each item is a posting locations dictionary for a bucket. The\n",
        "      posting locations maintain a list for each word of file locations and \n",
        "      offsets its posting list was written to. See `write_a_posting_list` for \n",
        "      more details.\n",
        "    '''\n",
        "    res = postings.map(lambda w_p: (token2bucket_id(w_p[0]),w_p))\n",
        "    res = res.groupByKey()\n",
        "    return res.map(lambda b_w_l : InvertedIndex.write_a_posting_list(b_w_l, bucket_name, ftype=ftype))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a423085",
      "metadata": {
        "id": "3a423085"
      },
      "outputs": [],
      "source": [
        "_ = partition_postings_and_write(postings_filtered_title, ftype='Title_index').collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f8d5a21",
      "metadata": {
        "id": "9f8d5a21"
      },
      "source": [
        "### Merge to one dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3fd6f3b",
      "metadata": {
        "id": "c3fd6f3b"
      },
      "outputs": [],
      "source": [
        "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
        "    if blob.name.endswith(\"pickle\"):\n",
        "        print(blob.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15da9ea0",
      "metadata": {
        "id": "15da9ea0"
      },
      "outputs": [],
      "source": [
        "super_posting_locs_title = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "        continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "        posting_locs = pickle.load(f)\n",
        "        for k, v in posting_locs.items():\n",
        "            super_posting_locs_title[k].extend(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6257fed3",
      "metadata": {
        "id": "6257fed3"
      },
      "source": [
        "### Write Index pkl to disc:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d161dbd3",
      "metadata": {
        "id": "d161dbd3"
      },
      "outputs": [],
      "source": [
        "title_docs_tokenized = doc_title_pairs.map(lambda x:(x[1], tokenize(x[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56ebb237",
      "metadata": {
        "id": "56ebb237"
      },
      "outputs": [],
      "source": [
        "# Create inverted index instance\n",
        "inverted_title = InvertedIndex(docs = title_docs_tokenized.collectAsMap())\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "inverted_title.posting_locs = super_posting_locs_title\n",
        "# Add the token - df dictionary to the inverted index\n",
        "inverted_title.df = w2df_title_dict\n",
        "# write the global stats out\n",
        "inverted_title.write_index('.', 'Title_index')\n",
        "\n",
        "# upload to gs\n",
        "index_src = \"Title_index.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33191ac1",
      "metadata": {
        "id": "33191ac1"
      },
      "source": [
        "### reading_index - just to check that everything is good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e447fbd",
      "metadata": {
        "id": "0e447fbd"
      },
      "outputs": [],
      "source": [
        "readed_index = InvertedIndex().read_index('.', 'Title_index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "119e54d6",
      "metadata": {
        "id": "119e54d6"
      },
      "outputs": [],
      "source": [
        "TUPLE_SIZE = 6\n",
        "TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n",
        "from contextlib import closing\n",
        "\n",
        "def read_posting_list(inverted, w):\n",
        "    with closing(MultiFileReader()) as reader:\n",
        "        locs = inverted.posting_locs[w]\n",
        "        b = reader.read(locs, inverted.df[w] * TUPLE_SIZE)\n",
        "        posting_list = []\n",
        "        for i in range(inverted.df[w]):\n",
        "            doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n",
        "            tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n",
        "            posting_list.append((doc_id, tf))\n",
        "        return posting_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f949b0",
      "metadata": {
        "id": "14f949b0"
      },
      "outputs": [],
      "source": [
        "read_posting_list(readed_index, 'leppänen_t')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e08ec6b5",
      "metadata": {
        "id": "e08ec6b5"
      },
      "source": [
        "## Now - index for the anchors: (for binary index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80938042",
      "metadata": {
        "id": "80938042",
        "outputId": "b4c2d703-240d-4a69-a74e-dd66e9efbc7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "doc_list_of_acnchors = parquetFile.select(\"id\", \"anchor_text\").rdd "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb2b998f",
      "metadata": {
        "id": "fb2b998f"
      },
      "source": [
        "### join anchor link to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e0a522a",
      "metadata": {
        "id": "2e0a522a",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-index_construction",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "page_pointer_text = doc_list_of_acnchors.flatMap(lambda x:x[1]).reduceByKey(lambda x,y:x + '. ' + y)\n",
        "#cast doc_anchor_pairs to be the same format as - title/body\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import SQLContext\n",
        "doc_anchor_pairs = page_pointer_text.map(lambda x: Row(anchor_text=x[1], id=x[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58fc2a7d",
      "metadata": {
        "id": "58fc2a7d"
      },
      "source": [
        "### Word counts - Term frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da0e9fde",
      "metadata": {
        "id": "da0e9fde",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-index_const_time",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "word_counts_anchors = doc_anchor_pairs.flatMap(lambda x: word_count(x[0], x[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9db8e197",
      "metadata": {
        "id": "9db8e197"
      },
      "source": [
        "### Posting list - Reduce and sort word count to get the posting list :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f5e8557",
      "metadata": {
        "id": "5f5e8557",
        "nbgrader": {
          "grade": true,
          "grade_id": "collect-posting",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "postings_anchors = word_counts_anchors.groupByKey().mapValues(sort_word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31e28a2f",
      "metadata": {
        "id": "31e28a2f"
      },
      "outputs": [],
      "source": [
        "postings_filtered_anchors = postings_anchors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89543834",
      "metadata": {
        "id": "89543834"
      },
      "source": [
        "### add _a at the end of anchors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f5bca6",
      "metadata": {
        "id": "95f5bca6"
      },
      "outputs": [],
      "source": [
        "postings_filtered_anchors = postings_filtered_anchors.map(lambda x:(x[0]+'_a', x[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5913b0d",
      "metadata": {
        "id": "d5913b0d"
      },
      "source": [
        "### Document frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0501e1ae",
      "metadata": {
        "id": "0501e1ae"
      },
      "outputs": [],
      "source": [
        "w2df_anchors = calculate_df(postings_filtered_anchors)\n",
        "w2df_anchors_dict = w2df_anchors.collectAsMap()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1a68636",
      "metadata": {
        "id": "d1a68636"
      },
      "source": [
        "### Write Index pkl to disc:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6772189",
      "metadata": {
        "id": "f6772189"
      },
      "outputs": [],
      "source": [
        "anchor_docs_tokenize = doc_anchor_pairs.map(lambda x: (x[1], tokenize(x[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eebb86fa",
      "metadata": {
        "id": "eebb86fa"
      },
      "source": [
        "### partition_postings_and_write"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dce0dd99",
      "metadata": {
        "id": "dce0dd99"
      },
      "outputs": [],
      "source": [
        "_ = partition_postings_and_write(postings_filtered_anchors, ftype='Anchor_index').collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96f2f031",
      "metadata": {
        "id": "96f2f031"
      },
      "source": [
        "### Merge to one dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "324649a1",
      "metadata": {
        "id": "324649a1"
      },
      "outputs": [],
      "source": [
        "super_posting_locs_anchors = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "        continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "        posting_locs = pickle.load(f)\n",
        "        for k, v in posting_locs.items():\n",
        "            super_posting_locs_anchors[k].extend(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dfe2aa0",
      "metadata": {
        "id": "0dfe2aa0"
      },
      "outputs": [],
      "source": [
        "# Create inverted index instance\n",
        "inverted_anchors = InvertedIndex()\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "inverted_anchors.posting_locs = super_posting_locs_anchors\n",
        "# Add the token - df dictionary to the inverted index\n",
        "inverted_anchors.df = w2df_anchors_dict\n",
        "# write the global stats out\n",
        "inverted_anchors.write_index('.', 'Anchor_index')\n",
        "\n",
        "# upload to gs\n",
        "index_src = \"Anchor_index.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef7f663b",
      "metadata": {
        "id": "ef7f663b"
      },
      "outputs": [],
      "source": [
        "# readed_index = InvertedIndex().read_index('postings_gcp/postings_gcp', 'Anchor_index') ## read "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10da03c9",
      "metadata": {
        "id": "10da03c9"
      },
      "source": [
        "# Now - body index: (for tfidf index - cosine similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcaba41d",
      "metadata": {
        "id": "bcaba41d"
      },
      "outputs": [],
      "source": [
        "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e46ca2",
      "metadata": {
        "id": "b4e46ca2"
      },
      "source": [
        "### Word counts - Term frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81d35d07",
      "metadata": {
        "id": "81d35d07"
      },
      "outputs": [],
      "source": [
        "word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7de990b8",
      "metadata": {
        "id": "7de990b8"
      },
      "source": [
        "### Posting list - Reduce and sort word count to get the posting list :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c836e65",
      "metadata": {
        "id": "4c836e65"
      },
      "outputs": [],
      "source": [
        "postings = word_counts.groupByKey().mapValues(sort_word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8dc72c9",
      "metadata": {
        "id": "f8dc72c9"
      },
      "source": [
        "### Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c176a17f",
      "metadata": {
        "id": "c176a17f"
      },
      "outputs": [],
      "source": [
        "# filter out rare words, words that appear in threshold(=50) or fewer documents\n",
        "threshold = 50\n",
        "postings_filtered = postings.filter(lambda x: len(x[1])>threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "347ae786",
      "metadata": {
        "id": "347ae786"
      },
      "source": [
        "### Document frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08f7aa5f",
      "metadata": {
        "id": "08f7aa5f"
      },
      "outputs": [],
      "source": [
        "# # get df\n",
        "w2df = calculate_df(postings_filtered)\n",
        "w2df_dict = w2df.collectAsMap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9167a92b",
      "metadata": {
        "id": "9167a92b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "corpus_len = parquetFile.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8d69c53",
      "metadata": {
        "id": "d8d69c53"
      },
      "source": [
        "### take df dict and replace df scores with idf scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f81e90dc",
      "metadata": {
        "id": "f81e90dc"
      },
      "outputs": [],
      "source": [
        "idf_dict = {token:np.log10(corpus_len/val) for token,val in w2df_dict.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0159157d",
      "metadata": {
        "id": "0159157d"
      },
      "source": [
        "### Tokeniz docs to insert to index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "173b6ca7",
      "metadata": {
        "id": "173b6ca7"
      },
      "outputs": [],
      "source": [
        "ls_tokens_filtered = postings_filtered.map(lambda x:x[0]).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f05ed8c3",
      "metadata": {
        "id": "f05ed8c3"
      },
      "outputs": [],
      "source": [
        "def get_filtered_tokens(token_list):\n",
        "    #input - document text as list of tokens\n",
        "    #output - the list of tokens when taking only the tokens who appear in postings_filtered\n",
        "    return [token for token in token_list if token in ls_tokens_filtered]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "793e0f44",
      "metadata": {
        "id": "793e0f44"
      },
      "outputs": [],
      "source": [
        "body_docs_tokenized = doc_text_pairs.map(lambda x: (x[1], tokenize(x[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ce4217b",
      "metadata": {
        "id": "7ce4217b"
      },
      "outputs": [],
      "source": [
        "# body_docs_tokenized = body_docs_tokenized.map(lambda x: (x[0], get_filtered_tokens(x[1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f922fb",
      "metadata": {
        "id": "16f922fb"
      },
      "outputs": [],
      "source": [
        "# doc = body_docs_tokenized.collectAsMap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaba84a6",
      "metadata": {
        "id": "aaba84a6"
      },
      "outputs": [],
      "source": [
        "# # Create inverted index instance\n",
        "inverted_tfidf = InvertedIndex()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35eee978",
      "metadata": {
        "id": "35eee978"
      },
      "outputs": [],
      "source": [
        "DL_dict = defaultdict(int)\n",
        "for i in body_docs_tokenized.collect():\n",
        "    DL_dict[i[0]] = i[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac06214b",
      "metadata": {
        "id": "ac06214b"
      },
      "outputs": [],
      "source": [
        "inverted_tfidf.DL = DL_dict "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0aebcf4",
      "metadata": {
        "id": "a0aebcf4"
      },
      "outputs": [],
      "source": [
        "#calculate posting list with tf*idf instead of tf -\n",
        "tfidf_counts = word_counts.filter(lambda x: x[0] in idf_dict)\n",
        "\n",
        "# d = 10**6\n",
        "\n",
        "\n",
        "def idf_product(x, d, idf_dict):\n",
        "    #input x - (token, (doc_id, tf))\n",
        "    #otput x - (token, (doc_id, tf*idf)) where tfidf is rounded by d digits\n",
        "    token = x[0]\n",
        "    doc_id = x[1][0]\n",
        "    tf = x[1][1]/DL_dict[doc_id]\n",
        "    tf_idf = tf*idf_dict[token]\n",
        "    rounded_tf_idf = int(np.round(tf_idf*10**d, d)) #*10**d to get integer\n",
        "    return token, (doc_id, rounded_tf_idf)\n",
        "\n",
        "tfidf_counts = tfidf_counts.map(lambda x: idf_product(x, 6, idf_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "440f9e1c",
      "metadata": {
        "id": "440f9e1c"
      },
      "outputs": [],
      "source": [
        "#group tfidf_counts to get posting list of (token - list(doc_id, tf*idf, doc_id, tf*idf))\n",
        "tfidf_postings = tfidf_counts.groupByKey().mapValues(sort_word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72da092f",
      "metadata": {
        "id": "72da092f"
      },
      "outputs": [],
      "source": [
        "#write posting lists to bin files, add idf to the name\n",
        "_ = partition_postings_and_write(tfidf_postings, ftype='Body_index').collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2df09ba",
      "metadata": {
        "id": "f2df09ba"
      },
      "outputs": [],
      "source": [
        "super_posting_locs_tfidf = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "        continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "        posting_locs = pickle.load(f)\n",
        "        for k, v in posting_locs.items():\n",
        "            super_posting_locs_tfidf[k].extend(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fa74a17",
      "metadata": {
        "id": "7fa74a17"
      },
      "source": [
        "### Calculate norm for each document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a63c20e9",
      "metadata": {
        "id": "a63c20e9"
      },
      "outputs": [],
      "source": [
        "# if we want to read the index from disk\n",
        "# read_inverted_tfidf = InvertedIndex().read_index('postings_gcp/postings_gcp', 'Body_index') \n",
        "# read_posting_list(inverted_tfidf, 'movement')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f997dfc",
      "metadata": {
        "id": "6f997dfc"
      },
      "outputs": [],
      "source": [
        "corpus_len = parquetFile.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cba4387f",
      "metadata": {
        "id": "cba4387f"
      },
      "outputs": [],
      "source": [
        "idf_dict = {token:np.log10(corpus_len/val) for token,val in read_inverted_tfidf.df.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0999f7f8",
      "metadata": {
        "id": "0999f7f8"
      },
      "outputs": [],
      "source": [
        "def product_idf(x):\n",
        "    token = x[0]\n",
        "    doc_id, tf = x[1]\n",
        "    tfidf = tf*idf_dict[token] if token in idf_dict else 0 #4.493417 = average value \n",
        "    return (doc_id, tfidf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfed1353",
      "metadata": {
        "id": "cfed1353"
      },
      "outputs": [],
      "source": [
        "word_doc_tfidf = word_counts.map(product_idf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66434097",
      "metadata": {
        "scrolled": true,
        "id": "66434097"
      },
      "outputs": [],
      "source": [
        "doc_tfidf_ls = word_doc_tfidf.groupByKey().mapValues(list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb15fcc3",
      "metadata": {
        "id": "cb15fcc3"
      },
      "outputs": [],
      "source": [
        "doc_norm_dict = doc_tfidf_ls.map(lambda x:(x[0], LA.norm(x[1]))).collectAsMap()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b75fd9e",
      "metadata": {
        "id": "0b75fd9e"
      },
      "source": [
        "Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "918bb2d8",
      "metadata": {
        "id": "918bb2d8"
      },
      "outputs": [],
      "source": [
        "# Adding the posting locations dictionary to the inverted index\n",
        "inverted_tfidf.posting_locs = super_posting_locs_tfidf\n",
        "# Add the token - df dictionary to the inverted index\n",
        "inverted_tfidf.df = w2df_dict\n",
        "#add dictionary of norm for each document\n",
        "inverted_tfidf.doc_norm_dict = doc_norm_dict\n",
        "# write the global stats out\n",
        "inverted_tfidf.write_index('.', 'Body_index')\n",
        "\n",
        "# upload to gs\n",
        "index_src = \"Body_index.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The final index - bm25 index: (for all sections - body, title, anchor)"
      ],
      "metadata": {
        "id": "Q4TnfkPU7FjG"
      },
      "id": "Q4TnfkPU7FjG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###first - get the exist indices until now, in order to make this calculations after the indices before already been readed."
      ],
      "metadata": {
        "id": "zBUahypw755P"
      },
      "id": "zBUahypw755P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63b964bd",
      "metadata": {
        "id": "63b964bd"
      },
      "outputs": [],
      "source": [
        "inverted_tfidf = InvertedIndex().read_index('postings_gcp/postings_gcp/', 'Body_index')\n",
        "inverted_anchors = InvertedIndex().read_index('postings_gcp/postings_gcp/', 'Anchor_index')\n",
        "inverted_title = InvertedIndex().read_index('postings_gcp/postings_gcp/', 'Title_index')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###calculate idf component (diifrent from cosine idf) "
      ],
      "metadata": {
        "id": "Bft17d8m8bu8"
      },
      "id": "Bft17d8m8bu8"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "28621664",
      "metadata": {
        "id": "28621664"
      },
      "outputs": [],
      "source": [
        "corpus_len = parquetFile.count()\n",
        "\n",
        "def idf_bm25(x, w2df):\n",
        "    #input x - (token, posting_list(doc_id, tf))\n",
        "    #otput x - (token, (doc_id, tf*idf)) where tfidf is rounded by d digits\n",
        "    token = x[0]\n",
        "    n_ti = w2df[token]\n",
        "    N = corpus_len\n",
        "    idf = np.log(((N-n_ti+0.5)/(n_ti+0.5))+1)\n",
        "    # rounded_idf = int(np.round(idf*10**d, d)) #*10**d to get integer\n",
        "    return (token, idf)#\n",
        "\n",
        "\n",
        "idf_bm25_body = postings_filtered.map(lambda x: (idf_bm25(x, w2df_dict)))\n",
        "idf_bm25_anchors = postings_filtered_anchors.map(lambda x: idf_bm25(x, w2df_anchors_dict))\n",
        "idf_bm25_title = postings_filtered_title.map(lambda x: idf_bm25(x, w2df_title_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e949737",
      "metadata": {
        "id": "6e949737"
      },
      "outputs": [],
      "source": [
        "#get the entire dictionary for each section\n",
        "bm25_idf_body_dict = idf_bm25_body.collectAsMap()\n",
        "bm25_idf_anchors_dict = idf_bm25_anchors.collectAsMap()\n",
        "bm25_idf_title_dict = idf_bm25_title.collectAsMap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b73176bd",
      "metadata": {
        "id": "b73176bd"
      },
      "outputs": [],
      "source": [
        "#calculate DL for anchors:\n",
        "dl_rdd_anc = doc_anchor_pairs.map(lambda x:(x[1],len(x[0].split())))\n",
        "dl_anc = defaultdict(int)\n",
        "for i in dl_rdd_anc.collect():\n",
        "    dl_anc[i[0]] = i[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10608cad",
      "metadata": {
        "id": "10608cad"
      },
      "outputs": [],
      "source": [
        "DL_body = inverted_tfidf.DL\n",
        "DL_anchors = dl_anc\n",
        "DL_title = inverted_title.DL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0da07184",
      "metadata": {
        "id": "0da07184"
      },
      "outputs": [],
      "source": [
        "#calculate avgdl: \n",
        "def calc_avgdl(DL):\n",
        "    sum_dl = 0\n",
        "    for val in DL.values():\n",
        "        sum_dl+=val\n",
        "    avgdl = sum_dl/len(DL)\n",
        "    return avgdl\n",
        "\n",
        "avgdl_body = calc_avgdl(DL_body)\n",
        "avgdl_anchors = calc_avgdl(DL_anchors)\n",
        "avgdl_title = calc_avgdl(DL_title)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###BM 25 calculation functions:"
      ],
      "metadata": {
        "id": "JIZA8WC-9Ecl"
      },
      "id": "JIZA8WC-9Ecl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ff16d1b",
      "metadata": {
        "id": "6ff16d1b"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calc_bm25(x, idf_dict, DL, avgdl):\n",
        "    #   Best Match 25.    \n",
        "    # ----------\n",
        "    # k1 : float, default 1.5\n",
        "    # b : float, default 0.75\n",
        "    b = 0.75\n",
        "    k1 = 1.5\n",
        "    token = x[0]\n",
        "    posting_ls = x[1] \n",
        "    postring_bm25_ls = []\n",
        "    for doc_id, tf in posting_ls:\n",
        "        mone = tf*(k1+1)\n",
        "    \n",
        "        mehane = tf+k1*(1-b+b*(DL[doc_id]/avgdl))\n",
        "        idf_token = idf_dict[token]\n",
        "        bm25_score = (idf_token*mone)/mehane\n",
        "        bm25_score_rounded = int(np.round(bm25_score*10**2, 0))\n",
        "        postring_bm25_ls.append((doc_id, bm25_score_rounded))\n",
        "    return token, postring_bm25_ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6249f816",
      "metadata": {
        "id": "6249f816"
      },
      "outputs": [],
      "source": [
        "postings_bm25_body = postings_filtered.map(lambda x: calc_bm25(x, bm25_idf_body_dict, DL_body, avgdl_body))\n",
        "postings_bm25_anchors = postings_filtered_anchors.map(lambda x: calc_bm25(x, bm25_idf_anchors_dict, DL_anchors, avgdl_anchors))\n",
        "postings_bm25_title = postings_filtered_title.map(lambda x: calc_bm25(x, bm25_idf_title_dict, DL_title, avgdl_title))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Union all to get one rdd"
      ],
      "metadata": {
        "id": "ID-f0LFc9JxX"
      },
      "id": "ID-f0LFc9JxX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d0cc85a",
      "metadata": {
        "id": "6d0cc85a"
      },
      "outputs": [],
      "source": [
        "union_posting = postings_bm25_body.union(postings_bm25_anchors).union(postings_bm25_title)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###write to disk"
      ],
      "metadata": {
        "id": "BA4SXZR89qkk"
      },
      "id": "BA4SXZR89qkk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8322f6e0",
      "metadata": {
        "id": "8322f6e0"
      },
      "outputs": [],
      "source": [
        "_ = partition_postings_and_write(union_posting, ftype=\"BM25\").collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x6fIaFDQ9s7P"
      },
      "id": "x6fIaFDQ9s7P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0625ab9d",
      "metadata": {
        "id": "0625ab9d"
      },
      "outputs": [],
      "source": [
        "posting_locs_bm25_union = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "        continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "        posting_locs = pickle.load(f)\n",
        "        for k, v in posting_locs.items():\n",
        "            posting_locs_bm25_union[k].extend(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4479cc9b",
      "metadata": {
        "id": "4479cc9b"
      },
      "outputs": [],
      "source": [
        "w2df_dict_super_duper['leppänen_t']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e7fb2ce",
      "metadata": {
        "id": "1e7fb2ce"
      },
      "outputs": [],
      "source": [
        "w2df_dict_super_duper = w2df_dict\n",
        "w2df_dict_super_duper.update(w2df_title_dict)\n",
        "w2df_dict_super_duper.update(w2df_anchors_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "379e9e5f",
      "metadata": {
        "id": "379e9e5f"
      },
      "outputs": [],
      "source": [
        "# Create inverted index instance\n",
        "inverted_bm25 = InvertedIndex()\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "inverted_bm25.posting_locs = posting_locs_bm25_union\n",
        "# # Add the token - df dictionary to the inverted index\n",
        "inverted_bm25.df = w2df_dict_super_duper\n",
        "# write the global stats out\n",
        "inverted_bm25.write_index('.', 'BM25_index')\n",
        "\n",
        "\n",
        "# upload to gs\n",
        "index_src = \"BM25_index.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Thats it, All the indices are in the disk"
      ],
      "metadata": {
        "id": "d_aYrJ0C99zx"
      },
      "id": "d_aYrJ0C99zx"
    },
    {
      "cell_type": "markdown",
      "id": "e832e9f0",
      "metadata": {
        "id": "e832e9f0"
      },
      "source": [
        "### Mount files to drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2043372",
      "metadata": {
        "id": "e2043372"
      },
      "outputs": [],
      "source": [
        "!gsutil ls -lh $index_dst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "419476ff",
      "metadata": {
        "id": "419476ff"
      },
      "outputs": [],
      "source": [
        "!gsutil ls gs://'207024878'/postings_gcp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cda63176",
      "metadata": {
        "id": "cda63176"
      },
      "outputs": [],
      "source": [
        "!mkdir postings_gcp\n",
        "!gsutil -m cp -r gs://'207024878'/postings_gcp/ \"postings_gcp/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "670da723",
      "metadata": {
        "id": "670da723",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2a6d655c112e79c5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# PageRank:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca43bd2f",
      "metadata": {
        "id": "ca43bd2f",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2fee4bc8d83c1e2a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Compute PageRank for the entire English Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cc27924",
      "metadata": {
        "id": "4cc27924",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-PageRank",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "t_start = time()\n",
        "pages_links = spark.read.parquet(\"gs://wikidata_preprocessed/*\").select(\"id\", \"anchor_text\").rdd\n",
        "# construct the graph \n",
        "edges, vertices = generate_graph(pages_links)\n",
        "# compute PageRank\n",
        "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
        "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
        "g = GraphFrame(verticesDF, edgesDF)\n",
        "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
        "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
        "pr = pr.sort(col('pagerank').desc())\n",
        "pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n",
        "pr_time = time() - t_start\n",
        "pr.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PageViews:"
      ],
      "metadata": {
        "id": "9Bi9yrN0-mGx"
      },
      "id": "9Bi9yrN0-mGx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "# Using user page views (as opposed to spiders and automated traffic) for the \n",
        "# month of August 2021\n",
        "pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n",
        "p = Path(pv_path) \n",
        "pv_name = p.name\n",
        "pv_temp = f'{p.stem}-4dedup.txt'\n",
        "pv_clean = f'{p.stem}.pkl'\n",
        "# Download the file (2.3GB) \n",
        "!wget -N $pv_path\n",
        "# Filter for English pages, and keep just two fields: article ID (3) and monthly \n",
        "# total number of page views (5). Then, remove lines with article id or page \n",
        "# view values that are not a sequence of digits.\n",
        "!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n",
        "# Create a Counter (dictionary) that sums up the pages views for the same \n",
        "# article, resulting in a mapping from article id to total page views.\n",
        "wid2pv = Counter()\n",
        "with open(pv_temp, 'rt') as f:\n",
        "  for line in f:\n",
        "    parts = line.split(' ')\n",
        "    wid2pv.update({int(parts[0]): int(parts[1])})\n",
        "# write out the counter as binary file (pickle it)\n",
        "with open(pv_clean, 'wb') as f:\n",
        "  pickle.dump(wid2pv, f)\n",
        "# # read in the counter\n",
        "# with open(pv_clean, 'rb') as f:\n",
        "#   wid2pv = pickle.loads(f.read())"
      ],
      "metadata": {
        "id": "6KDm4YXN-rI2"
      },
      "id": "6KDm4YXN-rI2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "collapsed_sections": [],
      "name": "preproccessing_gcp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}